{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">SnapChat Filters : Real-time FaceSwap</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we discussed how to get rid of the difference in lighting and skin tone between the source and destination images using Seamless Cloning. If you check again, you will notice that seamless cloning takes substantial amount of time to execute. So, it cannot be used for videos for achieving real-time face swap. We will use a different approach to blend the images as well as use the optimizations we had discussed in the previous sections to speed up te landmark detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Blending for Video based FaceSwap</font>\n",
    "\n",
    "Performing Seamless cloning on an 600x800 size image takes ~500ms for a single image. Taking other computations like delaunay triangulation and warping into consideration makes the frame rate < 2 FPS which is not acceptable for videos. Thus, we need an alternative method for performing FaceSwap in Videos.\n",
    "\n",
    "The blending is performed in two steps.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Color Correction</font>\n",
    "First, we apply color correction ( simple idea of [RGB Scaling](https://en.wikipedia.org/wiki/Color_balance#Scaling_monitor_R.2C_G.2C_and_B) ) so that the colors in the source image are matched to that of the destination image. Here are the steps:\n",
    "1. Simply blur both the images \n",
    "1. Take the ratio of the blurred Destination image to Source image. \n",
    "1. Multiply this ratio with the source image. \n",
    "\n",
    "This has an effect of scaling the source R, G, B values to match that of the destination image.\n",
    "\n",
    "\n",
    "\n",
    "| <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-blurredSrc.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-blurredSrc.jpg\"/></a></center> |  <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-blurredDst.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-blurredDst.jpg\"/></a></center> | <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-beforeCorrection.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-beforeCorrection.jpg\"/></a></center> |<center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-blurredDst.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-afterCorrection.jpg\"/></a></center>|\n",
    "| -------- | -------- | -------- |--|\n",
    "|<center>Blurred Src Image</center> | <center>Blurred Dst Image</center>     | <center>Before Color Correction</center>     |<center>After Color Correction</center>|\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Alpha Blending</font>\n",
    "Second, we perform alpha blending on the color corrected image. The foreground mask for alpha blending is taken as the warped convex hull of the source image and the background mask is just the inverse of that. The masks are blurred by a gaussian filter which helps in removing the seams that appear at the edges.\n",
    "\n",
    "| <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-ForegroundMask.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-ForegroundMask.jpg\"/></a></center> |  <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-BackgroundMask.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-BackgroundMask.jpg\"/></a></center> | <center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-afterCorrection.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-afterCorrection.jpg\"/></a></center> |<center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-afterBlending.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w5-m2-afterBlending.jpg\"/></a></center>|\n",
    "| -------- | -------- | -------- |--|\n",
    "|<center>Foreground Mask</center> | <center>Background Mask</center>     | <center>Before Blending</center>     |<center>After Blending</center>|\n",
    "\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Code and Tutorial for Real-time Face swap</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import faceBlendCommon as fbc\n",
    "from dataPath import DATA_PATH\n",
    "import colorCorrection as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8.0,8.0)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "matplotlib.rcParams['image.interpolation'] = 'bilinear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and initialize the landmark detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dlib facial landmark detector variables\n",
    "modelPath = DATA_PATH + \"models/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# initialize the dlib facial landmakr detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters for speeding up the face and landmark detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_FRAMES = 2\n",
    "FACE_DOWNSAMPLE_RATIO = 1.5\n",
    "RESIZE_HEIGHT = 480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the source image for swapping the face in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing input file\n",
    "filename1 = DATA_PATH + \"images/hillary_clinton.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find landmarks of the source image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image and resize it\n",
    "img1 = cv2.imread(filename1)\n",
    "height, width = img1.shape[:2]\n",
    "IMAGE_RESIZE = np.float32(height)/RESIZE_HEIGHT\n",
    "img1 = cv2.resize(img1,None,\n",
    "                 fx=1.0/IMAGE_RESIZE,\n",
    "                 fy=1.0/IMAGE_RESIZE,\n",
    "                 interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "# Find landmark points\n",
    "points1 = fbc.getLandmarks(detector, predictor, cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), FACE_DOWNSAMPLE_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find convex Hull of the source image and perform Delaunay Triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.float32(img1)\n",
    "\n",
    "# Find convex hull for delaunay triangulation using the landmark points\n",
    "hull1 = []\n",
    "hullIndex = cv2.convexHull(np.array(points1),clockwise=False, returnPoints = False)\n",
    "# addPoints = [[48],[49],[50],[51],[52],[53],[54],[55],[56],[57],[58]]\n",
    "# hullIndex = np.concatenate((hullIndex,addPoints))\n",
    "for i in range(0, len(hullIndex)):\n",
    "    hull1.append(points1[hullIndex[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find delanauy traingulation for convex hull points\n",
    "sizeImg1 = img1.shape\n",
    "rect = (0, 0, sizeImg1[1], sizeImg1[0])\n",
    "dt = fbc.calculateDelaunayTriangles(rect, hull1)\n",
    "\n",
    "if len(dt) == 0:\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process input from webcam or video file\n",
    "cap = cv2.VideoCapture(DATA_PATH + \"videos/sample-video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center> <a href=\"https://www.dropbox.com/s/mdkz86l1y345zkz/hillary_clinton.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/mdkz86l1y345zkz/hillary_clinton.jpg?dl=1\"/></a></center> |  <center> <a href=\"https://www.dropbox.com/s/953q1mlsk0bn3rn/image_28.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/953q1mlsk0bn3rn/image_28.jpg?dl=1\"/></a></center> |\n",
    "| -------- | -------- |\n",
    "|<center>Source Image</center> | <center>Target Frame</center>     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables for helping with stabilization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables for tracking time\n",
    "count = 0\n",
    "fps = 30.0\n",
    "tt = time.time()\n",
    "isFirstFrame = False\n",
    "sigma = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this loop we follow the following steps:\n",
    "\n",
    "1. Read a frame\n",
    "1. Get Landmarks for each frame ( Skip frames if required )\n",
    "1. Create convex hull using the landmarks around the face\n",
    "1. Stabilize the landmarks using optical flow tracking\n",
    "1. Warp the calculated delaunay triangles from source image to target frame\n",
    "1. Use the color correction method described above on the warped image\n",
    "1. Use alpha blending to get the final output.\n",
    "\n",
    "You can check out the intermediate outputs given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main loop\n",
    "while(True):\n",
    "\n",
    "    ret, img2 = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "      # Read each frame\n",
    "      height, width = img2.shape[:2]\n",
    "      IMAGE_RESIZE = np.float32(height)/RESIZE_HEIGHT\n",
    "      img2 = cv2.resize(img2,None,\n",
    "                         fx=1.0/IMAGE_RESIZE,\n",
    "                         fy=1.0/IMAGE_RESIZE,\n",
    "                         interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "      # find landmarks after skipping SKIP_FRAMES number of frames\n",
    "      if (count % SKIP_FRAMES == 0):\n",
    "        points2 = fbc.getLandmarks(detector, predictor, cv2.cvtColor(img2, cv2.COLOR_BGR2RGB), FACE_DOWNSAMPLE_RATIO)\n",
    "\n",
    "      # convert  float data type\n",
    "      img1Warped = np.copy(img2)\n",
    "      img1Warped = np.float32(img1Warped)\n",
    "\n",
    "      # if face is partially detected\n",
    "      if (len(points2) != 68 ):\n",
    "        continue\n",
    "\n",
    "      # Find convex hull\n",
    "      hull2 = []\n",
    "      for i in range(0, len(hullIndex)):\n",
    "        hull2.append(points2[hullIndex[i][0]])\n",
    "\n",
    "      ################ Optical Flow and Stabilization Code #####################\n",
    "      img2Gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "      if(isFirstFrame == False):\n",
    "        isFirstFrame = True\n",
    "        hull2Prev = np.array(hull2, np.float32)\n",
    "        img2GrayPrev = np.copy(img2Gray)\n",
    "\n",
    "      lk_params = dict( winSize  = (101,101),maxLevel = 5,criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.001))\n",
    "      hull2Next, st , err = cv2.calcOpticalFlowPyrLK(img2GrayPrev, img2Gray, hull2Prev, np.array(hull2,np.float32),**lk_params)\n",
    "\n",
    "      # Final landmark points are a weighted average of detected landmarks and tracked landmarks\n",
    "      for k in range(0,len(hull2)):\n",
    "        d = cv2.norm(np.array(hull2[k]) - hull2Next[k])\n",
    "        alpha = math.exp(-d*d/sigma)\n",
    "        hull2[k] = (1 - alpha) * np.array(hull2[k]) + alpha * hull2Next[k]\n",
    "        hull2[k] = fbc.constrainPoint(hull2[k], img2.shape[1], img2.shape[0])\n",
    "\n",
    "      # Update varibales for next pass\n",
    "      hull2Prev = np.array(hull2, np.float32)\n",
    "      img2GrayPrev = img2Gray\n",
    "      ################ End of Optical Flow and Stabilization Code ###############\n",
    "\n",
    "      # Warp the triangles\n",
    "      for i in range(0, len(dt)):\n",
    "        t1 = []\n",
    "        t2 = []\n",
    "\n",
    "        for j in range(0, 3):\n",
    "          t1.append(hull1[dt[i][j]])\n",
    "          t2.append(hull2[dt[i][j]])\n",
    "\n",
    "        fbc.warpTriangle(img1, img1Warped, t1, t2)\n",
    "\n",
    "      ##################  Blending  #############################################\n",
    "      img1Warped = np.uint8(img1Warped)\n",
    "      cv2.imshow(\"img1Warped\", img1Warped)\n",
    "\n",
    "      # Color Correction of the warped image so that the source color matches that of the destination\n",
    "      output = cc.correctColours(img2, img1Warped, points2)\n",
    "\n",
    "      cv2.imshow(\"After color correction\", output)\n",
    "\n",
    "      # Create a Mask around the face\n",
    "      re = cv2.boundingRect(np.array(hull2,np.float32))\n",
    "      centerx = (re[0]+(re[0]+re[2]))/2\n",
    "      centery = (re[1]+(re[1]+re[3]))/2\n",
    "\n",
    "      hull3 = []\n",
    "      for i in range(0,len(hull2)):\n",
    "        # Take the points just inside of the convex hull\n",
    "        hull3.append((0.95*(hull2[i][0] - centerx) + centerx, 0.95*(hull2[i][1] - centery) + centery))\n",
    "\n",
    "      mask1 = np.zeros((img2.shape[0], img2.shape[1],3), dtype=np.float32)\n",
    "      hull3Arr = np.array(hull3,np.int32)\n",
    "\n",
    "      cv2.fillConvexPoly(mask1,hull3Arr,(255.0,255.0,255.0),16,0)\n",
    "\n",
    "      # Blur the mask before blending\n",
    "      mask1 = cv2.GaussianBlur(mask1,(21,21),10)\n",
    "\n",
    "      mask2 = (255,255,255) - mask1\n",
    "\n",
    "      cv2.imshow(\"mask1\", np.uint8(mask1))\n",
    "      cv2.imshow(\"mask2\", np.uint8(mask2))\n",
    "\n",
    "      # Perform alpha blending of the two images\n",
    "      temp1 = np.multiply(output,(mask1*(1.0/255)))\n",
    "      temp2 = np.multiply(img2,(mask2*(1.0/255)))\n",
    "      result = temp1 + temp2\n",
    "\n",
    "      cv2.imshow(\"temp1\", np.uint8(temp1))\n",
    "      cv2.imshow(\"temp2\", np.uint8(temp2))\n",
    "\n",
    "      result = np.uint8(result)\n",
    "\n",
    "      cv2.imshow(\"After Blending\", result)\n",
    "      if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "      count += 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center> <a href=\"https://www.dropbox.com/s/953q1mlsk0bn3rn/image_28.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/953q1mlsk0bn3rn/image_28.jpg?dl=1\"/></a></center> |  <center> <a href=\"https://www.dropbox.com/s/qw5wfdclmgo8whp/img1Warped_28.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/qw5wfdclmgo8whp/img1Warped_28.jpg?dl=1\"/></a></center> |\n",
    "| -------- | -------- |\n",
    "|<center>Target Image</center> | <center>Warped Image</center> |\n",
    "\n",
    "| <center> <a href=\"https://www.dropbox.com/s/b4ye1cy7t3cwkde/After_color_correction_28.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/b4ye1cy7t3cwkde/After_color_correction_28.jpg?dl=1\"/></a></center> |<center> <a href=\"https://www.dropbox.com/s/0z2bzp2a0g0cu73/After_Blending_28.jpg?dl=1\"><img src = \"https://www.dropbox.com/s/0z2bzp2a0g0cu73/After_Blending_28.jpg?dl=1\"/></a></center>|\n",
    "| -------- | -------- |\n",
    "| <center>After Color Correction</center>     | <center>Final Result after Blending</center>     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">References and Further Reading</font>\n",
    "\n",
    "1. [http://www.learnopencv.com/face-swap-using-opencv-c-python/](http://www.learnopencv.com/face-swap-using-opencv-c-python/)\n",
    "\n",
    "2. [https://matthewearl.github.io/2015/07/28/switching-eds-with-python/](https://matthewearl.github.io/2015/07/28/switching-eds-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
